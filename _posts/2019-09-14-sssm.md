---
layout: post
title: Switching State Space Models and Structured Variational Inference
---
![alt text](https://semihakbayrak.github.io/images/sssm.jpeg "SSSM")
Used in broad range of areas from robotics to finance, State Space Models (a.k.a. Kalman filters) assume a stationary generative process of temporal data. Some temporal data, on the other hand, are generated by switching regimes through time. For example, a mechanical part of a wind turbine could be tracked well by only one Kalman filter if the wind speed were constant. However, wind turbines should be designed such a way that the behaviour of mechanical parts are segmented according to speed and direction of wind for the sake of efficient energy transformation. So, detection of wind regime is crucial in wind turbine design. Switching State Space Models (SSSMs) address this kind of non-stationary behaviours with mixture of Kalman filters. Depicted in the figure above, an SSSM is a composition of Kalman filter and hidden Markov models (HMMs). To be more precise, the generative model: 

$$ 
p(\mathbf{y}_{1:T},x_{1:T},z_{1:T}) = p(z_1)p(x_1|z_1)p(y_1|x_1,z_1)\prod\limits_{t=2}^{T}{p(z_t|z_{t-1})p(x_t|x_{t-1},z_t)p(y_t|x_t,z_t)} 
$$.

Let's take a closer look at the conditional distributions, starting with the continuous latent variables:

$$
p(x_t|x_{t-1},z_t) = \mathcal{N}(x_t;A_tx_{t-1},V_t)
$$,

where $A_t \in \left\{A^{k}\right\}_{k=1}^{K}$ and $k=z_t$.

As it can be imagined, exact inference for such a model is intractable. So, we resort to variational bound optimization with some relaxations in the recognition distribution. The most relaxed recognition model choice is the one all the random variables are factorized. However, it is a better practice to stick to the original model factorization in order to keep variational objective tighter to the log-likelihood.
