---
layout: post
title: Online Natural Gradient as a Kalman Filter
---

Gradient descent and its stochastic variants are widely used iterative methods in learning tasks. Although it is guaranteed that gradient descent converges to a local mininma if the step size is arranged properly, its performance is highly dependent on parameterization of the likelihood distribution. Moreover, it does not utilize the curveture of the manifold. Natural gradient descent, on the other hand, runs optimization in distribution space instead of parameter space and does not possess the shortcomings of gradient descent. Its stochastic version can be formulated as an extended Kalman filter which further provides step size values.

![alt text](https://semihakbayrak.github.io/images/NGIF.png "Factor Graph Representation")

I've prepared a Jupyter notebook that contains more details and visualizations. Check [here] (https://github.com/semihakbayrak/MachineLearning/blob/master/Notes/Online%20Natural%20Gradient%20as%20a%20Kalman%20Filter.ipynb)
